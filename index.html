<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    figure figcaption {
    color: 	#A9A9A9;
    text-align: center;
	}
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .three
    {
    width: 80px;
    height: 80px;
    position: relative;
    }
    .four
    {
    width: 80px;
    height: 80px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <title>Lele Chen</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="70%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Lele Chen   &nbsp; &nbsp;   <img src="name.png" width="120" height="50"> </name>
        </p>
        <p>I am a Ph.D candidate advised by <a href="https://www.cs.rochester.edu/~cxu22/">Prof. Chenliang Xu</a> at <a href="https://www.cs.rochester.edu/">University of Rochester</a>, where I work on computer vision and mechine learning. My research interests are multimodal modeling and video object detection/segmentation.
        </p>
        <p>
           I received my M.S. degree from <a href="https://www.cs.rochester.edu/">University of Rochester</a> in 2018. I've spent time at <a href="https://www.visualdx.com/">visualDx Medical Image Lab</a> and <a href="http://corporate.jd.com/">JD.com JDX Autonomous Driving Lab</a> as R&D intern.
        </p>
        <p align=center>
          <a href="email.txt">Email</a> &nbsp/&nbsp
          <a href="Lele_cv.pdf">CV</a> &nbsp/&nbsp
          <a href="Lele_bio.txt">Biography</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=H71yt54AAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/lele-chen-a14353136/"> LinkedIn </a>
        </p>
        </td>
        <td width="33%">
        	<figure>
			  <img src="Lele.jpg" width="380" height="380">
			  <figcaption> @Vienna, May. 2015  </figcaption>
			</figure>
        
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
          I'm interested in computer vision, machine learning, optimization and image processing. Much of my research is about inferring other modulities (language, audio, lidar point cloud ) to images. I have also worked in autonomous driving.
          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

<!-- #######################Project 0####################################### -->

    <tr onmouseout="hdrnet_stop()" onmouseover="hdrnet_start()" >
      <td width="25%">
        <div class="one">
        <div class="two" id = 'cvpr2019'><img src='cvpr2019.gif' width="180" height="180"></div>
        <img src='cvpr2019.gif' width="180" height="180">
        </div>
        <script type="text/javascript">
        function hdrnet_start() {
        document.getElementById('cvpr2019').style.opacity = "1";
        }
        function hdrnet_stop() {
        document.getElementById('cvpr2019').style.opacity = "0";
        }
        hdrnet_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="cvpr2019.pdf">
        <papertitle>Hierarchical Cross-modal Talking Face Generation with Dynamic Pixel-wise Loss</papertitle></a><br>
        <strong>Lele Chen</strong>, <a href="https://www.urmc.rochester.edu/people/30363779-ross-k-maddox">Ross K. Maddox</a>, <a href="http://www2.ece.rochester.edu/~zduan/">Zhiyao Duan </a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu </a> <br>
        <em>CVPR</em>, 2019 <br>  
        <a href="cvpr2019.pdf">paper</a>
        /
        <a href="">video (soon)</a>
        /
        <a href="LeleCVPR2019.bib">bibtex</a>
        /
        <a href="https://github.com/lelechen63/ATVGnet">code</a>
        <p></p>
        <p>We devise a cascade GAN approach to generate talking face video, which is robust to different face shapes, view angles, facial characteristics, and noisy audio conditions. Instead of learning a direct mapping from audio to video frames, we propose first to transfer audio to high-level structure, i.e., the facial landmarks, and then to generate video frames conditioned on the landmarks. Compared to a direct audio-to-image approach, our cascade approach avoids fitting spurious correlations between audiovisual signals that are irrelevant to the speech content. We, humans, are sensitive to temporal discontinuities and subtle artifacts in video. To avoid those pixel jittering problems and to enforce the network to focus on audiovisual-correlated regions, we propose a novel dynamically adjustable pixel-wise loss with an attention mechanism.</p>
      </td>
    </tr>
<!-- #######################Project 1####################################### -->

    <tr onmouseout="hdrnet_stop()" onmouseover="hdrnet_start()" >
      <td width="25%">
        <div class="one">
        <div class="two" id = 'eccv2018'><img src='eccv2018.gif'></div>
        <img src='eccv2018.gif'>
        </div>
        <script type="text/javascript">
        function hdrnet_start() {
        document.getElementById('eccv2018').style.opacity = "1";
        }
        function hdrnet_stop() {
        document.getElementById('eccv2018').style.opacity = "0";
        }
        hdrnet_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="1140.pdf">
        <papertitle>Lip Movements Generation at a Glance</papertitle></a><br>
        <strong>Lele Chen</strong>, Zhiheng Li, <a href="https://www.urmc.rochester.edu/people/30363779-ross-k-maddox">Ross K Maddox</a>, <a href="http://www2.ece.rochester.edu/~zduan/">Zhiyao Duan </a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu </a> <br>
        <em>ECCV</em>, 2018 <br>
        <a href="1140.pdf">paper</a>
        /
        <a href="1140_poster.pdf">poster</a>
        /
        <a href="https://www.youtube.com/watch?v=mmI31GdGL5g">video</a>
        /
        <a href="LeleECCV2018.bib">bibtex</a>
        /
        <a href="https://shiropen.com/seamless/lip-movements-generation-at-a-glance">news</a>
        /
        <a href="https://github.com/lelechen63/3d_gan">code</a>
        <p></p>
        <p>Given an arbitrary audio speech and one lip image of arbitrary target identity, generate synthesized lip movements of the target identity saying the speech. To perform well, a model needs to not only consider the retention of target identity, photo-realistic of synthesized images, consistency and smoothness of lip images in a sequence, but more importantly, learn the correlations between audio speech and lip movements.</p>
      </td>
    </tr>

<!-- #######################Project 2####################################### -->

    <tr onmouseout="hdrnet_stop()" onmouseover="hdrnet_start()" >
      <td width="25%">
        <div class="one">
        <div class="two" id = 'acmmm2017'><img src='acmmm2017.gif'></div>
        <img src='acmmm2017.gif'>
        </div>
        <script type="text/javascript">
        function hdrnet_start() {
        document.getElementById('acmmm2017').style.opacity = "1";
        }
        function hdrnet_stop() {
        document.getElementById('acmmm2017').style.opacity = "0";
        }
        hdrnet_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="acmmm2017.pdf">
        <papertitle>Deep cross-modal audio-visual generation</papertitle></a><br>
        <strong>Lele Chen</strong>, Sudhanshu Srivastava, <a href="http://www2.ece.rochester.edu/~zduan/">Zhiyao Duan </a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu </a> <br>
        <em>ACM MMW</em>, 2017 <br>
        <a href="acmmm2017.pdf">paper</a>
        /
        <a href="acmmm_poster.pdf">poster</a>
        /
        <a href="ChenSDX17.bib">bibtex</a>
        /
        <a href="https://github.com/lelechen63/3d_gan">code</a>
        <p></p>
        <p>We have developed algorithms in audio-visual source association that are able to segment corresponding audio-visual data pairs; we have created deep generative neural networks utilizing adversarial training that are able to generate one modality, i.e., audio/visual, from the other modality, i.e., visual/audio. The outputs of cross-modal generation are beneficial to many applications, such as aiding hearing- or visually-impaired and content creation in virtual reality.</p>
      </td>
    </tr>

<!-- #######################Project 3####################################### -->

    <tr onmouseout="hdrnet_stop()" onmouseover="hdrnet_start()" >
      <td width="25%">
        <div class="one">
        <div class="two" id = 'spie2017'><img src='spie2017.gif'></div>
        <img src='spie2017.gif'>
        </div>
        <script type="text/javascript">
        function hdrnet_start() {
        document.getElementById('spie2017').style.opacity = "1";
        }
        function hdrnet_stop() {
        document.getElementById('spie2017').style.opacity = "0";
        }
        hdrnet_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="spie2017.pdf">
        <papertitle>MRI Tumor Segmentation with Densely Connected 3D CNN</papertitle></a><br>
        <strong>Lele Chen</strong>, Yue Wu, <a href="https://www.rochester.edu/college/gradstudies/profiles/adora-dsouza.html">Adora M. DSouza </a>, Anas Z. Abidin, <a href="https://www.urmc.rochester.edu/people/27063859-axel-w-e-wismueller">Axel Wismuller</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu </a> <br>
        <em>SPIE Image Processing</em>, 2017 <br>
        <a href="spie2017.pdf">paper</a>
        /
        <a href="spie_slides.pdf">slides</a>
        /
        <a href="Chenspie2017.bib">bibtex</a>
        /
        <a href="https://github.com/lelechen63/MRI-tumor-segmentation-Brats">code</a>
        <p></p>
        <p>In this paper, we introduce a new approach for brain tumor segmentation in MRI scans. DenseNet was initially introduced for the image classification problem. In this work, we explore the potential of densely connected blocks in 3D segmentation tasks. Compared with traditional networks with no skip connections, the improved information flow extracts better features and significantly help the optimization. We take multi-scale receptive fields into account to accurately classify voxels.</p>
      </td>
    </tr>

<!-- #######################Project 4####################################### -->

    <tr onmouseout="hdrnet_stop()" onmouseover="hdrnet_start()" >
      <td width="25%">
        <div class="one">
        <div class="two" id = 'asa2018'><img src='asa2018.gif'></div>
        <img src='asa2018.gif'>
        </div>
        <script type="text/javascript">
        function hdrnet_start() {
        document.getElementById('jasa2018').style.opacity = "1";
        }
        function hdrnet_stop() {
        document.getElementById('jasa2018').style.opacity = "0";
        }
        hdrnet_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://asa.scitation.org/doi/abs/10.1121/1.5035944">
        <papertitle>Toward a visual assistive listening device: Real-time synthesis of a virtual talking face from acoustic speech using deep neural networks</papertitle></a><br>
        <strong>Lele Chen</strong>, Emre Eskimez, Zhiheng Li, <a href="http://www2.ece.rochester.edu/~zduan/">Zhiyao Duan </a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu </a>, <a href="https://www.urmc.rochester.edu/people/30363779-ross-k-maddox">Ross K Maddox</a> <br>
        <em>The Journal of the Acoustical Society of America</em>, 2018 <br>
        <a href="https://asa.scitation.org/doi/abs/10.1121/1.5035944">paper</a>
        /
        <a href="CHENasa2018.bib">bibtex</a>
        <p></p>
        <p>Speech perception is a crucial function of the human auditory system, but speech is not only an acoustic signal-visual cues from a talker's face and articulators (lips, teeth, and tongue) carry considerable linguistic information. These cues offer substantial and important improvements to speech comprehension when the acoustic signal suffers degradations like background noise or impaired hearing. However, useful visual cues are not always available, such as when talking on the phone or listening to a podcast. We are developing a system for generating a realistic speaking face from speech audio input. The system uses novel deep neural networks trained on a large audio-visual speech corpus. It is designed to run in real time so that it can be used as an assistive listening device. Previous systems have shown improvements in speech perception only for the most degraded speech. Our design differs notably from earlier ones in that it does not use a language model-instead, it makes a direct transformation from speech audio to face video. This allows the temporal coherence between the acoustic and visual modalities to be preserved, which has been shown to be crucial to cross-modal perceptual binding.</p>
      </td>
    </tr>


<!-- ##################################Teaching######################## -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Teaching</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="100%" valign="center">
        <p>
          <a href="http://www.simon.rochester.edu/faculty-and-research/conferences/itt-conference2/agenda/download.aspx?id=16634">
          <papertitle>CIS442F Big Data - Spring 2018</papertitle></a>
          <p> This class offers an introduction to big data concepts, environments, processes, and tools from the perspective of data analysts and data scientists. The course will set the scene for the emergence of big data as an important trend in the business world and explain the technical architectures that make analyzing data at scale possible. The hands-on portion of the class will focus on the major tools of the Hadoop big data ecosystem such as HDFS, Pig, Hive, Sqoop, Hue, Zeppelin, and Spark. In addition, students will gain a broad understanding of the role of MapReduce, Tez, Impala, YARN, and other big data technologies.</p>
          <br><br>
          <papertitle>GBA 464  Programming for Analytics - Fall 2017</papertitle>
          <p>This course provides some foundations for programming in the R environment. We cover traditional programming concepts such as operators, data structures, control structures, repetition and user-defined functions. In addition, these concepts will be taught in the context of marketing and business analytics problems related to data management and visualization. Other than high-level programming, the students will gain a foundational understanding of how data is can be stored, organized and pulled, in given data analytics context. </p>
          <br>
        </p>
        </td>
      </tr>
      </table>



<!-- ##################################other######################## -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Other</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="100%" valign="center">
        <p>
          <a href="https://www.cs.rochester.edu/u/nelson/courses/csc_400/csc_400.html">
          <papertitle>CSC400  Graduate Problem Seminar - Fall 2018</papertitle></a>
          <p> You can find my NSF ITRG Miniproposal in <a href="NSFProposal.pdf">
          <papertitle>here</papertitle></a> .</p>  
          <br>
        </p>
        </td>
      </tr>
      </table>
      <table width="25%" align="center" border="0" cellspacing="0" cellpadding="20">
      <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=7Zzchk2flXT0aRBt5Ct1_jG-vG0KJgX8x2_aoCGIIQc&cl=ffffff&w=a"></script>
      </table>

      <HR>
      <CENTER>
      <A HREF="/users"><IMG SRC="/images/up.gif" ALT="UP" BORDER=0></A>
      <A HREF="/"><IMG SRC="/images/home.gif" ALT="HOME" BORDER=0></A>
      <BR>
      <A HREF="/users">URCS People</A> |
      <A HREF="/">URCS Home Page</A>
      </CENTER>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
           <a href="https://github.com/jonbarron/jonbarron_website"><strong>this guy's website is awesome</strong></>
	  </font>
        </p>
        </td>
      </tr>
      </table>
      
    </td>
    </tr>
  </table>
</body>
</html>
